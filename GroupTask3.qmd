---
title: "Group Task 3: Hurricane Risk Assessment for Gulf of Mexico Cities"
author:
  - name: Yves-Langston Mays
    email: ymmays@cougarnet.uh.edu
    affiliation:
      - name: University of Houston Main Campus
        department: Natural Sciences & Mathematics
        city: Houston
        country: USA
  - name: Uyen Vi Phan
    email: uphan2@uh.edu
    affiliation:
      - name: University of Houston Main Campus
        department: Natural Sciences & Mathematics
        city: Houston
        country: USA
  - name: Ny Dang
    email: tndang8@cougarnet.uh.edu
    affiliation:
      - name: University of Houston Main Campus
        department: Natural Sciences & Mathematics
        city: Houston
        country: USA
  - name: Connie Yang
    email: cyang39@cougarnet.uh.edu
    affiliation:
      - name: University of Houston Main Campus
        department: Natural Sciences & Mathematics
        city: Houston
        country: USA
---
abstract:
  "Task 3 centers on hurricane risk for 25 cities in the Gulf of Mexico, using data from the Atlantic hurricane database (HURDAT2) from 1851 to 2023, provided by the National Hurricane Center. To analyze and assess this risk, we will perform 3 analyses in R to assess the hurricane risk on the Gulf of Mexico. First, we visualize and note our findings on the storm tracks over the last 25 years (1999-2024), focusing on storm paths, intensity, and duration at each location. Spatial correlation analysis will be used to explore the relationship between hurricane occurrences and contributing environmental factors, while Non-Parametric Density Estimation will estimate location-specific risk based on historical hurricane trajectories. These analyses collectively aim to identify the cities at highest risk of hurricane impact and gauge potential severity."
keywords: [Non-Parametric Density Estimation, Spatial Correlation Analysis, Hurricane Risk Assessment]
format:
  pdf:
    template: GroupTask3.tex
    pdf-engine: lualatex
    keep-tex: true
    highlight-style: none
    template-partials:
      - before-body.tex
    toc: true
    number-sections: true
    cite-method: biblatex
bibliography: references.bib
---

#  Introduction

One of the most common natural disaster plaguing the Gulf of Mexico are Hurricanes. Just this year (2024) there has been 9 hurricanes in the Atlantic Ocean including Beryl, Helen and Milton. These storms can have lasting impacts to people's lives, the environment, infrastructure and to the economy. Since 1980, hurricane damage has costed over \$1.3 trillion in damages with an average of \$22.8 billion dollars per event and 6,890 deaths \[1\]. Learning to predict where, when, and the intensity of hurricanes can not only save us billions of dollars but thousands of lives as well. This is especially important for cities that are at high risk such as cities like New Orleans that exists at extremely low elevations.

This task aims to use historical data on hurricanes to predict future hurricane activity and highlight cities that are at most risk. The historical hurricane data from the National Hurricane Center’s HURDAT2 database, contains hurricane data in the Atlantic from 1851 to 2023. HURDAT2 records the six-hourly information on the location, maximum winds, central pressure, and (beginning in 2004) size of all known tropical cyclones and subtropical cyclones\[2\]. Along with a list of 25 cities and their locations in the Gulf of Mexico, 3 analyses will be performed oh HURDAT2 to analyze and predict the storm tracks in the Gulf of Mexico.

The last 25 (1999-2024) years worth of storm tracks will be first visualized and analyzed on the over the Gulf of Mexico to identify common patterns and trends of the storm tracks. The visualization and the manual analysis provides a good general overview of how these storms move, where they are the most intense, and what places receive the most storms.

The second analysis examines how certain factors like sea surface temperatures and El Nino/La Nina patterns affect hurricane activity. Spatial correlation analysis between these factors and hurricane activity will be used to achieve this. The correlation analysis will show how certain weather phenomena can affect hurricane activity to better predict hurricane activity.

The last analysis uses non-parametric density estimation to assess the hurricane risk based on past trajectories and severity. This can provide insight on where hurricanes are more likely to appear and what routes they take. It can assess what regions will receive the most intense storms. Paired with the spatial correlation analysis, the information provided from the non-parametric density estimation can highlight what cities are most at risk in the Gulf of Mexico.

These 3 analyses can provide valuable insight to the behavior and activity of future hurricanes. Knowing the behavior and patterns of the hurricanes, knowing what cities and regions are at the most risk from hurricane activity and how different factors play a role in hurricane activity can help people better prepare for hurricanes and minimize the losses caused by these storms. Meteorologists can use these predictions to better inform people about the route, severity of storms and what to expect as it passes. Governments can use this information to predict what cities will require the most aid. Insurance companies can use this data to decide what services are best suited to a specific location.


#  Background

The data source, HURDAT2, short for Hurricane Database 2 from the National Hurricane Center, provides detailed hurricane tracks from 1851 to 2023. This database records measurements of critical parameters including location coordinates, maximum wind speeds, and central pressure. Since 2004, the database has also included storm size data, enhancing our understanding of hurricane characteristics.

Our report encompasses 25 cities across the Gulf of Mexico region, representing diverse geographical and political jurisdictions. Ten U.S. cities form the northern boundary of the study area, including major metropolitan centers like New Orleans, Houston, and Miami, alongside significant coastal cities such as Tampa, Corpus Christi, Pensacola, Mobile, Galveston, Biloxi, and Key West. Along the western and southern Gulf coast, nine Mexican cities are included: Veracruz, Tampico, and Campeche serve as major ports, while Cancún represents a vital tourism center. The study also includes Mérida, Ciudad del Carmen, Progreso, Coatzacoalcos, and Tuxpan, which are crucial to Mexico's coastal economy. The Caribbean region is represented by Havana, Varadero, and Cienfuegos, along with key island locations including Belize City, George Town, and Nassau.

Natural factors significantly influence hurricane activity in the Gulf region, with varying degrees of impact severity. Sea surface temperatures directly affect hurricane formation and intensification, while El Niño/La Niña patterns influence atmospheric conditions and storm frequency. The Atlantic Multidecadal Oscillation affects long-term hurricane activity cycles, and Saharan dust levels can suppress hurricane development. Upper-level wind patterns play a crucial role in determining storm trajectories and development potential.

Each city's unique geographical position and environmental characteristics contribute to its specific hurricane risk profile. This research aims to quantify specific hurricane risks for each city, identify correlations between environmental factors and hurricane patterns, and develop location-specific risk assessment models.

The integration of these methods below will support and inform decision-making processes for protecting coastal communities from future hurricane impacts.

#  Methodology

## 3.1 Data Collection and Preparation
Data preparation consists of converting HURDAT2 into a workable data for analysis. To achieve this, the storm name and ID are given its own columns. For the gulf cities data, 2 additional columns were made for its longitude and latitude.

```{r ImportingLibraries}
#| include: false
#libraries
library(zoo)
library(sf)
library(leaflet)
library(dplyr)
library(ggplot2)
```

```{r DataPrep}
#| include: false
#data collection and prep
hurdat2 = read.csv("hurdat2-1851-2023-051124.txt", header=F, as.is=T)

names(hurdat2) = c("DATE", "TIME_UTC", "POINT_TYPE", "STATUS", 
               "LATITUDE", "LONGITUDE", "WINDSPEED_KT", "PRESURE_MB", 
               "NE_34KT", "SE_34KT", "NW_34_KT", "SW_34_KT",
               "NE_50KT", "SE_50KT", "NW_50_KT", "SW_50_KT",
               "NE_64KT", "SE_64KT", "NW_64_KT", "SW_64_KT")

# this is the panel we need for the visualization
panel = cbind(HID = NA, HNAME = NA, hurdat2)

panel$HID = ifelse(grepl("AL|EP|CP", panel$DATE), panel$DATE, NA)

panel$HNAME = ifelse(grepl("AL|EP|CP", panel$DATE), panel$TIME_UTC, NA)

panel$HID = na.locf(panel$HID)

panel$HNAME = na.locf(panel$HNAME)

panel = panel[!grepl("AL|EP|CP", panel$DATE), ]


# these are the coordinates
panel$LATITUDE = trimws(panel$LATITUDE)
panel$LONGITUDE = trimws(panel$LONGITUDE)
panel$STATUS = trimws(panel$STATUS)

panel$LATITUDE = ifelse(grepl("S", panel$LATITUDE), paste0("-", panel$LATITUDE), panel$LATITUDE)
panel$LONGITUDE = ifelse(grepl("W", panel$LONGITUDE), paste0("-", panel$LONGITUDE), panel$LONGITUDE)

panel$LATITUDE = as.numeric(sub("N|S", "", panel$LATITUDE))
panel$LONGITUDE = as.numeric(sub("E|W", "", panel$LONGITUDE))


# gulf storms
gulf_storms = subset(panel, 
                    LATITUDE >= 18 & LATITUDE <= 30 & 
                    LONGITUDE >= -98 & LONGITUDE <= -80)

```

## 3.2 Geographic Data Setup

```{r DataPrep2}
#| echo: false
gulf_cities <- data.frame(
  City = c("New Orleans", "Houston", "Tampa", "Miami", "Corpus Christi", 
           "Pensacola", "Mobile", "Galveston", "Biloxi", "Key West",
           "Veracruz", "Tampico", "Campeche", "Cancún", "Mérida",
           "Ciudad del Carmen", "Progreso", "Coatzacoalcos", "Tuxpan", "Havana",
           "Varadero", "Cienfuegos", "Belize City", "George Town", "Nassau"),
  Country = c(rep("USA", 10), rep("Mexico", 9), rep("Cuba", 3), "Belize", "Cayman Islands", "Bahamas"),
  Latitude = c(30.0, 29.8, 28.0, 25.8, 27.8, 30.4, 30.7, 29.3, 30.4, 24.6,
               19.2, 22.2, 19.8, 21.2, 21.0, 18.7, 21.3, 18.1, 21.0, 23.1,
               23.2, 22.2, 17.5, 19.3, 25.0),
  Longitude = c(-90.1, -96.4, -82.5, -80.2, -97.4, -87.2, -88.0, -94.8, -88.9, -81.8,
                -96.1, -97.9, -90.5, -86.9, -89.6, -91.8, -89.7, -94.5, -97.4, -82.4,
                -81.2, -80.4, -88.2, -81.4, -77.4)
)

# boundaries for gulf of mexico region
gulf_bounds <- list(
  lat_min = min(gulf_cities$Latitude) - 1,  
  lat_max = max(gulf_cities$Latitude) + 1,
  lon_min = min(gulf_cities$Longitude) - 1,
  lon_max = max(gulf_cities$Longitude) + 1
)

gulf_storms <- subset(panel, 
                     LATITUDE >= gulf_bounds$lat_min & 
                     LATITUDE <= gulf_bounds$lat_max & 
                     LONGITUDE >= gulf_bounds$lon_min & 
                     LONGITUDE <= gulf_bounds$lon_max)


gulf_storms$YEAR <- as.numeric(substring(gulf_storms$DATE, 1, 4))
gulf_storms$MONTH <- as.numeric(substring(gulf_storms$DATE, 5, 6))


cat("Study Area Boundaries:\n")
cat("Latitude:", gulf_bounds$lat_min, "to", gulf_bounds$lat_max, "°N\n")
cat("Longitude:", gulf_bounds$lon_min, "to", gulf_bounds$lon_max, "°W\n")
cat("\nTotal storm observations:", nrow(gulf_storms))
cat("\nUnique storms:", length(unique(gulf_storms$HID)))
cat("\nDate range:", min(gulf_storms$DATE), "to", max(gulf_storms$DATE))

```

## 3.3 Visualization

```{r LeafletMap}
#| echo: false
# I will attempt to use leaflet correctly to create an interactive map
# Rendering this map is computationally expensive....

names(gulf_storms) <- ifelse(names(gulf_storms) == "" | is.na(names(gulf_storms)), paste0("V", seq_along(names(gulf_storms))), names(gulf_storms))

recent_gulf_storms <- gulf_storms %>%
  mutate(YEAR = as.numeric(substr(DATE, 1, 4))) %>%
  filter(YEAR >= 1999) %>%
  slice(seq(1, n(), by = 5))

leaflet(recent_gulf_storms) %>%
  addTiles() %>%
  setView(lng = -90, lat = 25, zoom = 5) %>%
  addCircleMarkers(
    lng = ~LONGITUDE,
    lat = ~LATITUDE,
    color = ~case_when(
      STATUS == "HU" ~ "red",
      STATUS == "TS" ~ "orange",
      TRUE ~ "blue"
    ),
    radius = 3,
    clusterOptions = markerClusterOptions(), # This line adds clustering to the points
    popup = ~paste(
      "Storm Name:", HNAME, "<br>",
      "Date:", DATE, "<br>",
      "Status:", STATUS, "<br>",
      "Wind Speed:", WINDSPEED_KT, "kt"
    )
  ) %>%
  addLegend(
    position = "bottomright",
    colors = c("red", "orange", "blue"),
    labels = c("Hurricane", "Tropical Storm", "Other"),
    title = "Storm Status"
  )


```

## 3.4 Statistical Analysis

```{r StatisticalAnalysis}
#| echo: false
yearly_storms <- recent_gulf_storms %>%
  group_by(YEAR) %>%
  summarize(storm_count = n())

View(yearly_storms)

monthly_storms <- recent_gulf_storms %>%
  mutate(MONTH = as.numeric(substr(DATE, 5, 6))) %>%
  group_by(MONTH) %>%
  summarize(storm_count = n())

View(monthly_storms)

status_count <- recent_gulf_storms %>%
  group_by(STATUS) %>%
  summarize(count = n())

View(status_count)

```

```{r Plots}
ggplot(yearly_storms, aes(x = YEAR, y = storm_count)) +
  geom_line(color = "blue") +
  geom_point() +
  labs(title = "year storm count from 1999-2023",
       x = "Year",
       y = "# of Storms")

ggplot(monthly_storms, aes(x = MONTH, y = storm_count)) +
  geom_col(fill = "steelblue") +
  labs(title = "monthly storm freq",
       x = "Month",
       y = "# of Storms") +
  scale_x_continuous(breaks = 1:12,
                     labels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", 
                                "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))

ggplot(status_count, aes(x = STATUS, y = count, fill = STATUS)) +
  geom_bar(stat = "identity") +
  labs(title = "strm count by status",
       x = "Status",
       y = "# of Storms") +
  scale_fill_manual(values = c("HU" = "red", "TS" = "orange", "TD" = "blue"))


intensity_trend <- recent_gulf_storms %>%
  group_by(YEAR) %>%
  summarize(avg_windspeed = mean(WINDSPEED_KT, na.rm = TRUE))

ggplot(intensity_trend, aes(x = YEAR, y = avg_windspeed)) +
  geom_line(color = "darkred") +
  geom_point() +
  labs(title = "Avg Storm Intensity Over Time 99-23",
       x = "Year",
       y = "Avg Wind Speed (kt)")



```

```{r}
#| echo: false
library(ncdf4)
library(dplyr)
library(tidyr)
library(sp)
library(corrplot)
library(purrr)

# Sea Surface Temperature (SST) data
sst_file <- nc_open("sst.mnmean.nc")
sst_data <- ncvar_get(sst_file, "sst")
sst_mean <- apply(sst_data, 1, mean, na.rm = TRUE)
sst_df <- data.frame(Year = 1950 + seq_along(sst_mean) - 1, SST = sst_mean)
nc_close(sst_file)

# El Niño/La Niña (ONI/ANOM) data
oni_data <- read.table("oni.ascii.txt", header = TRUE)
names(oni_data)[names(oni_data) == "YR"] <- "Year"

# Atlantic Multidecadal Oscillation (AMO) data
lines <- readLines("amon.sm.data")
numeric_lines <- lines[3:(length(lines) - 5)]
data_string <- paste(numeric_lines, collapse = "\n")
temp_file <- tempfile(fileext = ".txt")
writeLines(data_string, temp_file)
amo_data <- read.table(temp_file, header = TRUE, na.strings = "-99.990")
names(amo_data)[1] <- "Year"
amo_long <- pivot_longer(amo_data, cols = -Year, names_to = "Month", values_to = "AMO_Value")

# Saharan Dust Levels (AOT) data 
aot_files <- list.files(path = "aot", pattern = "*.nc", full.names = TRUE)
aot_data_list <- list()

for (aot_file in aot_files) {
    if (file.exists(aot_file)) {
        aot_nc <- nc_open(aot_file)
        aot_data <- ncvar_get(aot_nc, "AOD550")
        nc_close(aot_nc)
        
        aot_mean <- rowMeans(aot_data, na.rm = TRUE)
        year <- as.numeric(substr(basename(aot_file), 1, 4))
        month <- as.numeric(substr(basename(aot_file), 5, 6))
        
        aot_df <- data.frame(Year = rep(year, length(aot_mean)), Month = month, AOT = aot_mean)
        aot_data_list[[length(aot_data_list) + 1]] <- aot_df
    }
}

aot_combined <- bind_rows(aot_data_list) %>%
    group_by(Year) %>%
    summarize(AOT = mean(AOT, na.rm = TRUE))

# Upper-Level Wind Patterns (Uwind) data
uwnd_file <- nc_open("uwnd.mon.mean.nc")
uwnd_data <- ncvar_get(uwnd_file, "uwnd")
uwnd_mean <- apply(uwnd_data, 1, mean, na.rm = TRUE)
uwnd_df <- data.frame(Year = 1950 + seq_along(uwnd_mean) - 1, Uwind = uwnd_mean)
nc_close(uwnd_file)

# Merge with hurricane data
hurricane_activity <- gulf_storms %>%
    group_by(YEAR) %>%
    summarise(storm_count = n()) %>%
    rename(Year = YEAR)

combined_data <- list(sst_df, 
                      oni_data %>% group_by(Year) %>% summarize(TOTAL = mean(TOTAL, na.rm = TRUE), ANOM = mean(ANOM, na.rm = TRUE)), 
                      amo_long %>% group_by(Year) %>% summarize(AMO_Value = mean(AMO_Value, na.rm = TRUE)), 
                      aot_combined, 
                      uwnd_df) %>%
    reduce(left_join, by = "Year") %>%
    left_join(hurricane_activity, by = "Year")

# Filter to only include years up to 2024
combined_data <- combined_data %>%
    filter(Year <= 2024)
```

```{r}
# Histograms
numeric_columns <- sapply(combined_data, is.numeric)

par(mfrow = c(3, 3))  # Arrange plots in a 3x3 grid
for (col_name in names(combined_data)[numeric_columns]) {
    hist(combined_data[[col_name]], 
         main = paste("Histogram of", col_name),
         xlab = col_name, 
         breaks = 20, 
         col = "lightblue", 
         border = "black")
}

# Reset plotting layout for time series
par(mfrow = c(1, 1)) 

# Time Series Plots
time_series_vars <- c("SST", "AOT", "AMO_Value", "TOTAL", "storm_count", "Uwind")

for (var in time_series_vars) {
    if (var %in% names(combined_data)) {
        plot(combined_data$Year, combined_data[[var]], type = "l", col = "blue",
             main = paste(var, "Over Time"),
             xlab = "Year", 
             ylab = var)
        points(combined_data$Year, combined_data[[var]], pch = 19, col = "darkblue")
    }
}

# Correlation Analysis
correlation_matrix <- cor(combined_data[, c("storm_count", "SST", "TOTAL", "ANOM", "AMO_Value", "AOT", "Uwind")], use = "pairwise.complete.obs")
print(correlation_matrix)
corrplot(correlation_matrix, method = "circle")

# Regression models
variables_to_analyze <- c("SST", "TOTAL", "ANOM", "AMO_Value", "AOT", "Uwind")

for (var in variables_to_analyze) {
    current_data <- combined_data %>%
        filter(!is.na(.data[[var]]) & !is.na(storm_count))
    
    if (nrow(current_data) > 0) {
        model <- lm(as.formula(paste("storm_count ~", var)), data = current_data)
        cat("\n\nLinear Model for Storm Count ~", var, ":\n")
        print(summary(model))  # Print model summary
    }
}
```
The first chart shows the amount of storms that occur during each year from 1999 to 2023. There is no clear pattern for this, as the amount of storms per year is very variable. Storm count can be as low as 5 storms in a year to over 50 storms in a year.The second graph shows the frequency of storms by month. Storms tend to peak during the summer months, mainly August, September and October and is usually the lowest during winter to early spring, mainly December, January, February and March. This makes sense on a scientific standpoint as hurricanes are fueled by warm ocean water and summer is usually when the water is the warmest in the year. The third chart shows the frequency of storms based on their type:Tropical Wave, Tropical Depression, Tropical Storm, Hurricane, Extratropical Cyclone, Subtropical Depression, Subtropical Storm, Low Pressure System, and Non-Tropical Disturbance. Tropical storms being the most common followed by hurricanes and tropical depressions. Subtropical Depression is the least common.This could show that more intense storms are more common in the Gulf of Mexico, which can mean higher overall risk for cities that get frequent storms. The last chart shows the average wind speed per year. Most years, the average windspeed ranges between 40kt-60kt. With 2005 followed by 2004 to be the highest years with approximately 68kt and 66kt. The lowest year is 2015 with the average wind speed being 29kt. The average windspeeds after 2016 seems to increased overall as well.

## Non-parametric density estimation

Method 1: Kernel Density Estimation
```{r}
library(MASS)
library(fields)

kde <- kde2d(
  gulf_storms$LONGITUDE, 
  gulf_storms$LATITUDE,
  n = 100,
  lims = c(range(gulf_storms$LONGITUDE), range(gulf_storms$LATITUDE))
)

# risk scores for cities
city_risks <- interp.surface(
  list(x = kde$x, y = kde$y, z = kde$z),
  cbind(gulf_cities$Longitude, gulf_cities$Latitude)
)

# results dataframe
kde_results <- data.frame(
  City = gulf_cities$City,
  Risk_Score = city_risks
)

# normalize scores
kde_results$Risk_Score <- scale(kde_results$Risk_Score)

# sort by risk
kde_results <- kde_results[order(-kde_results$Risk_Score),]

# results
ggplot(kde_results, aes(x = reorder(City, Risk_Score), y = Risk_Score)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Hurricane Risk by City (KDE Method)",
       x = "City",
       y = "Risk Score")

print(kde_results)

```


Method 2: K-Nearest Neighbors
```{r}
library(geosphere)

k <- 30  # number of nearest storms to consider
risks <- sapply(1:nrow(gulf_cities), function(i) {
  # distances to all storms
  dists <- distHaversine(
    cbind(gulf_cities$Longitude[i], gulf_cities$Latitude[i]),
    cbind(gulf_storms$LONGITUDE, gulf_storms$LATITUDE)
  )
  
  # k nearest storms
  nearest <- order(dists)[1:k]
  mean(gulf_storms$WINDSPEED_KT[nearest])  # use average windspeed as risk measure
})

# results dataframe
knn_results <- data.frame(
  City = gulf_cities$City,
  Risk_Score = risks
)

# normalize scores
knn_results$Risk_Score <- scale(knn_results$Risk_Score)

# sort by risk
knn_results <- knn_results[order(-knn_results$Risk_Score),]

# results
ggplot(knn_results, aes(x = reorder(City, Risk_Score), y = Risk_Score)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Hurricane Risk by City (KNN Method)",
       x = "City",
       y = "Risk Score")

print(knn_results)
```


=======
\newpage

#  Results

Discuss results of analysis

\newpage

#  Discussion

Discuss the implications and significance here

\newpage

#  Conclusion

Conclusion Here

# References {.unnumbered}

::: {#refs}
\[1\] National Oceanic and Atmospheric Administration. 2023. Hurricane Costs. NOAA, Office for Coastal Management. Retrieved October 16, 2024, from <https://coast.noaa.gov/states/fast-facts/hurricane-costs.html>\[1\] National Oceanic and Atmospheric Administration. 2023. Hurricane Costs. NOAA, Office for Coastal Management. Retrieved October 16, 2024, from <https://coast.noaa.gov/states/fast-facts/hurricane-costs.html>

\[2\] National Hurricane Center. 2023. Data Archive. NOAA, National Oceanic and Atmospheric Administration. Retrieved October 16, 2024, from <https://www.nhc.noaa.gov/data/>

\[3\] NOAA Physical Sciences Laboratory. 2023. NOAA Extended Reconstructed Sea Surface Temperature (ERSST) v4. Retrieved November 7, 2024, from <https://downloads.psl.noaa.gov/Datasets/noaa.ersst.v4/.> Original data source: B. Huang, et al., Extended Reconstructed Sea Surface Temperature version 4 (ERSST.v4): Part I. Upgrades and Intercomparisons, J. Clim., 28, 911-930, 2015.

\[4\] NOAA Climate Prediction Center. 2023. Oceanic Niño Index (ONI). Retrieved November 7, 2024, from <https://www.cpc.ncep.noaa.gov/data/indices/oni.ascii.txt.>

\[5\] NOAA Physical Sciences Laboratory. 2023. Atlantic Multidecadal Oscillation (AMO) - Smoothed, Short (1948 to Jan 2023). Retrieved November 7, 2024, from <https://psl.noaa.gov/data/timeseries/AMO/>

\[6\] Copernicus Climate Change Service (C3S). 2023. Satellite Aerosol Properties Dataset. Retrieved November 7, 2024, from <https://cds.climate.copernicus.eu/datasets/satellite-aerosol-properties?tab=download.>

\[7\] NOAA Physical Sciences Laboratory. 2023. NCEP/NCAR Reanalysis Project. Retrieved November 7, 2024, from <https://psl.noaa.gov/data/gridded/data.ncep.reanalysis.html.> Original data source: R. Kalnay et al., The NCEP/NCAR 40-year reanalysis project, Bull. Amer. Meteor. Soc., 77, 437–470, 1996.
:::
